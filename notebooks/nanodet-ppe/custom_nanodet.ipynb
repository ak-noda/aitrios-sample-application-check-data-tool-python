{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L_czKlNX8Cw"
      },
      "source": [
        "# Train NanoDet with custom dataset\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/nanodet-ppe/custom_nanodet.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Training NanoDet model to detect Personal Protection Equipment (PPE) using open source dataset.\n",
        "\n",
        "Nanodet training based on https://github.com/RangiLyu/nanodet/tree/main\n",
        "\n",
        "Tutorial includes:\n",
        "- Dataset setup\n",
        "- Nanodet model setup\n",
        "- Training\n",
        "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ua5tQiESX8C0",
        "outputId": "6de254b2-cf2a-4d20-f9fb-4425ec2b10d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m237.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m284.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m251.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m189.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m292.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m202.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m262.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m338.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m244.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --no-cache-dir torch~=1.13.1 torchvision \"tensorflow>=2.14.0,<2.15.0\" pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "to4dEx8jX8C1",
        "outputId": "25f4b6e9-493d-4b5d-e35c-c7387bb2f2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shm memory: 5.68GB\n",
            "Is cuda available: True\n"
          ]
        }
      ],
      "source": [
        "# Perform initial checks in order to continue\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "assert '2.14' in tf.__version__, print(tf.__version__)\n",
        "assert '1.13' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "# Check shared memory\n",
        "shm_stats = shutil.disk_usage('/dev/shm')\n",
        "shm_in_gb = shm_stats.total / (1024 ** 3)\n",
        "print(f\"shm memory: {shm_in_gb:.2f}GB\")\n",
        "\n",
        "print(f'Is cuda available: {torch.cuda.is_available()}')\n",
        "assert shm_in_gb >= 12 or torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHcetx2qX8C1"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g52uZj_IX8C1",
        "outputId": "348a663c-6f77-4662-fbab-aae7265dbbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanodet'...\n",
            "remote: Enumerating objects: 2722, done.\u001b[K\n",
            "remote: Counting objects: 100% (616/616), done.\u001b[K\n",
            "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
            "remote: Total 2722 (delta 535), reused 485 (delta 485), pack-reused 2106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2722/2722), 5.28 MiB | 5.60 MiB/s, done.\n",
            "Resolving deltas: 100% (1618/1618), done.\n",
            "Note: switching to 'be9b4a9'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at be9b4a9 Replaced opencv with imagesize to get the actual image size (#548)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m255.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m233.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m313.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Known errors:\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
        "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "\"\"\"\n",
        "NANODET_COMMIT = 'be9b4a9'\n",
        "!rm -rf nanodet\n",
        "!git clone https://github.com/RangiLyu/nanodet.git\n",
        "!touch nanodet/nanodet/model/__init__.py\n",
        "!cd nanodet && git checkout {NANODET_COMMIT} && pip install -q --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnIqRvXCX8C2"
      },
      "source": [
        "# Dataset\n",
        "- go to https://universe.roboflow.com/ai-camp-safety-equipment-detection/ppe-detection-using-cv/dataset/3 and click `\"Download Dataset\"`\n",
        "- choose format `\"COCO\"` and `\"show download code\"` and `\"continue\"`\n",
        "- choose `\"Terminal\"` and copy the command `\"curl...\"` and paste the command in the cell below.\n",
        "- add `\"!\"` in the beginning of the command and replace `\"\\&gt;\"` with `\">\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5xyqiqmZX8C2",
        "outputId": "7c8bb68e-261a-4778-ea6e-5ec307f4dcba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   901  100   901    0     0   2160      0 --:--:-- --:--:-- --:--:--  2165\n",
            "100  235M  100  235M    0     0  13.8M      0  0:00:17  0:00:17 --:--:-- 17.4M\n"
          ]
        }
      ],
      "source": [
        "# Add below your download code from Roboflow, it should look like the following, with your unique roboflow dataset url:\n",
        "# Example (with \"!\" added in the beginning of the command and replaced \"&gt;\" with \">\". Also added \"-q\" for less output):\n",
        "!curl -L \"https://universe.roboflow.com/ds/4juCFTKWpx?key=HlRho6YNW4\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YhOOLAF4X8C2"
      },
      "outputs": [],
      "source": [
        "# Move test/train/valid to dataset folder\n",
        "from pathlib import Path\n",
        "DATASET_PATH = 'dataset/PPE_Detection_Using_CV.v3i.coco'\n",
        "if not Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists():\n",
        "    assert Path(f'train/_annotations.coco.json').exists()\n",
        "    assert Path(f'valid/_annotations.coco.json').exists()\n",
        "    assert Path(f'test/_annotations.coco.json').exists()\n",
        "    !mkdir -p $DATASET_PATH\n",
        "    !mv test train valid *txt $DATASET_PATH/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BU_bBK7EX8C3"
      },
      "outputs": [],
      "source": [
        "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists()\n",
        "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IOO1n5X8C3"
      },
      "source": [
        "# Training config file: nanodet-plus-m-1.5x_416-ppe.yml\n",
        "The following block of code creates the NanoDet training config file which\n",
        "is based on nanodet/config/nanodet-plus-m-1.5x_416.yml.\n",
        "Updated for the custom PPE dataset\n",
        "Change number of `total_epochs` for better performance.\n",
        "\n",
        "If training on GPU, then set   `gpu_ids`:\n",
        " * 1 gpu: [0]\n",
        " * 2 gpu: [0,1]\n",
        " * etc...\n",
        "\n",
        "Increase `total_epochs`, for example 20.\n",
        "\n",
        "Feel free to increase `val_intervals`, for example 10.\n",
        "\n",
        "For details see NanoDet github repo and [config docs](https://github.com/RangiLyu/nanodet/blob/main/docs/config_file_detail.md). Observe recommendation to adjust `lr` with `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AUMWJqW9X8C3"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "touch nanodet-plus-m-1.5x_416-ppe.yml\n",
        "cat <<EOF >nanodet-plus-m-1.5x_416-ppe.yml\n",
        "# Comments:\n",
        "# -  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "# -  \"device\": settings for colab T4 GPU\n",
        "# -  \"total_epochs\": set to 20 during testing, default 300\n",
        "save_dir: workspace/nanodet-plus-m-1.5x_416-ppe\n",
        "model:\n",
        "  weight_averager:\n",
        "    name: ExpMovingAverager\n",
        "    decay: 0.9998\n",
        "  arch:\n",
        "    name: NanoDetPlus\n",
        "    detach_epoch: 10\n",
        "    backbone:\n",
        "      name: ShuffleNetV2\n",
        "      model_size: 1.5x\n",
        "      out_stages: [2,3,4]\n",
        "      activation: LeakyReLU\n",
        "    fpn:\n",
        "      name: GhostPAN\n",
        "      in_channels: [176, 352, 704]\n",
        "      out_channels: 128\n",
        "      kernel_size: 5\n",
        "      num_extra_level: 1\n",
        "      use_depthwise: True\n",
        "      activation: LeakyReLU\n",
        "    head:\n",
        "      name: NanoDetPlusHead\n",
        "      num_classes: 8\n",
        "      input_channel: 128\n",
        "      feat_channels: 128\n",
        "      stacked_convs: 2\n",
        "      kernel_size: 5\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "      norm_cfg:\n",
        "        type: BN\n",
        "      loss:\n",
        "        loss_qfl:\n",
        "          name: QualityFocalLoss\n",
        "          use_sigmoid: True\n",
        "          beta: 2.0\n",
        "          loss_weight: 1.0\n",
        "        loss_dfl:\n",
        "          name: DistributionFocalLoss\n",
        "          loss_weight: 0.25\n",
        "        loss_bbox:\n",
        "          name: GIoULoss\n",
        "          loss_weight: 2.0\n",
        "    # Auxiliary head, only use in training time.\n",
        "    aux_head:\n",
        "      name: SimpleConvHead\n",
        "      num_classes: 8\n",
        "      input_channel: 256\n",
        "      feat_channels: 256\n",
        "      stacked_convs: 4\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "data:\n",
        "  train:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/train\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      perspective: 0.0\n",
        "      scale: [0.6, 1.4]\n",
        "      stretch: [[0.8, 1.2], [0.8, 1.2]]\n",
        "      rotation: 0\n",
        "      shear: 0\n",
        "      translate: 0.2\n",
        "      flip: 0.5\n",
        "      brightness: 0.2\n",
        "      contrast: [0.6, 1.4]\n",
        "      saturation: [0.5, 1.2]\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "  val:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "device:\n",
        "  gpu_ids: -1\n",
        "  workers_per_gpu: 2\n",
        "  batchsize_per_gpu: 32\n",
        "  precision: 32 # set to 16 to use AMP training\n",
        "schedule:\n",
        "#  resume:\n",
        "#  load_model:\n",
        "  optimizer:\n",
        "    name: AdamW\n",
        "    lr: 0.001\n",
        "    weight_decay: 0.05\n",
        "  warmup:\n",
        "    name: linear\n",
        "    steps: 500\n",
        "    ratio: 0.0001\n",
        "  total_epochs: 20\n",
        "  lr_schedule:\n",
        "    name: CosineAnnealingLR\n",
        "    T_max: 300\n",
        "    eta_min: 0.00005\n",
        "  val_intervals: 10\n",
        "grad_clip: 35\n",
        "evaluator:\n",
        "  name: CocoDetectionEvaluator\n",
        "  save_key: mAP\n",
        "log:\n",
        "  interval: 10\n",
        "\n",
        "class_names: [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-xXpIpWX8C4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "K2cv11CyX8C4"
      },
      "outputs": [],
      "source": [
        "# OBSERVE: update the following assert statement to match your yml file settings.\n",
        "\n",
        "import yaml\n",
        "with open('nanodet-plus-m-1.5x_416-ppe.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "assert config['device']['gpu_ids'] == -1, print(f\"gpu_ids: {config['device']['gpu_ids']}\")\n",
        "assert config['schedule']['total_epochs'] == 20, print(f\"total_epochs: {config['schedule']['total_epochs']}\")\n",
        "assert config['schedule']['val_intervals'] == 10, print(f\"val_intervals: {config['schedule']['val_intervals']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mOpDPE8_X8C4",
        "outputId": "d15b7129-1a80-4f21-a53d-2ff8ae313647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[12-21 14:23:06]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mSetting up data...\u001b[0m\n",
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[12-21 14:23:06]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mCreating model...\u001b[0m\n",
            "model size is  1.5x\n",
            "init weights...\n",
            "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1_5-3c479a10.pth\n",
            "Finish initialize NanoDet-Plus Head.\n",
            "\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[12-21 14:23:06]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mUsing CPU training\u001b[0m\n",
            "GPU available: True (cuda), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "  rank_zero_warn(\n",
            "\n",
            "  | Name      | Type        | Params\n",
            "------------------------------------------\n",
            "0 | model     | NanoDetPlus | 7.8 M \n",
            "1 | avg_model | NanoDetPlus | 7.8 M \n",
            "------------------------------------------\n",
            "15.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.5 M    Total params\n",
            "62.073    Total estimated model params size (MB)\n",
            "\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[12-21 14:23:07]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mWeight Averaging is enabled\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2024-12-21 14:23:40.756937: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-21 14:23:40.757051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-21 14:23:40.761558: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-21 14:23:43.538861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\u001b[35m[NanoDet]\u001b[0m\u001b[34m[12-21 14:23:46]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[97mTrain|Epoch1/20|Iter0(1/44)| mem:0G| lr:1.00e-07| loss_qfl:0.6601| loss_bbox:1.1392| loss_dfl:0.5221| aux_loss_qfl:0.6563| aux_loss_bbox:1.1004| aux_loss_dfl:0.5412| \u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "assert '1.13' in torch.__version__, print(torch.__version__)\n",
        "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists()\n",
        "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTzchEkdX8C4"
      },
      "source": [
        "# Remove aux layers that are only used during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MeumdtmSX8C4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"./nanodet\")\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "from nanodet.model.arch import build_model\n",
        "from nanodet.util import cfg, load_config, Logger\n",
        "\n",
        "def remove_aux(cfg, model_path, remove_layers=['aux_fpn', 'aux_head'], debug=False):\n",
        "    model = build_model(cfg.model)\n",
        "    ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "    if len(remove_layers) > 0:\n",
        "        state_dict = copy.deepcopy(ckpt['state_dict'])\n",
        "        for rlayer in remove_layers:\n",
        "            for layer in ckpt['state_dict']:\n",
        "                if rlayer in layer:\n",
        "                    del state_dict[layer]\n",
        "                    if debug:\n",
        "                        print(f'removed layer: {layer}')\n",
        "        del ckpt['state_dict']\n",
        "        ckpt['state_dict'] = copy.deepcopy(state_dict)\n",
        "        del state_dict\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KWzNzmzSX8C4",
        "outputId": "1f96375a-4543-402c-ac39-5df05720c40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size is  1.5x\n",
            "init weights...\n",
            "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1_5-3c479a10.pth\n",
            "Finish initialize NanoDet-Plus Head.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ef3d621d9ac0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'aux_fpn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aux_head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Saved to: {dst_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-959aa49c8406>\u001b[0m in \u001b[0;36mremove_aux\u001b[0;34m(cfg, model_path, remove_layers, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aux_fpn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aux_head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'"
          ]
        }
      ],
      "source": [
        "config_path = 'nanodet-plus-m-1.5x_416-ppe.yml'\n",
        "model_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'\n",
        "dst_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best-removed-aux.pth'\n",
        "\n",
        "load_config(cfg, config_path)\n",
        "ckpt = remove_aux(cfg, model_path, ['aux_fpn', 'aux_head'])\n",
        "torch.save(ckpt, dst_path)\n",
        "print(f'Saved to: {dst_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pVWgnrb7X8C5",
        "outputId": "63425057-6a4c-49ae-cb5a-bdcc4cdd132c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Compare size w and w/o aux\n",
        "!ls -l workspace/nanodet-plus-m-1.5x_416-ppe/model_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGgW5e8KX8C5"
      },
      "source": [
        "# Quantization of custom NanoDet model using Model Compression Toolkit\n",
        "Quantization is based on https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq/example_keras_nanodet_plus.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJDlRtseX8C5"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPlOINvBX8C5"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir -q \"tensorflow>=2.14.0,<2.15.0\" pycocotools\n",
        "import sys\n",
        "MCT_COMMIT = 'v2.1.0'\n",
        "!rm -rf local_mct\n",
        "!git clone https://github.com/sony/model_optimization.git local_mct && cd local_mct && git checkout {MCT_COMMIT} && pip install --no-cache-dir -r requirements.txt\n",
        "sys.path.insert(0,\"./local_mct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZyb8DNHX8C5"
      },
      "source": [
        "# Keras NanoDet float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y1JE8OoX8C5"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "assert '2.14' in tf.__version__, print(tf.__version__)\n",
        "assert '1.13' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "from keras.models import Model\n",
        "import model_compression_toolkit as mct\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_plus_m\n",
        "from tutorials.mct_model_garden.models_keras.utils.torch2keras_weights_translation import load_state_dict\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_box_decoding\n",
        "assert 'local_mct' in mct.__file__, print(mct.__file__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY3wkhDpX8C5"
      },
      "outputs": [],
      "source": [
        "# Upload the trained custom model\n",
        "CUSTOM_WEIGHTS_FILE = dst_path  # The NanoDet model trained with PPE dataset\n",
        "CLASS_NAMES = [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "DATASET_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train'\n",
        "ANNOT_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json'\n",
        "DATASET_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid'\n",
        "ANNOT_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json'\n",
        "DATASET_REPR = DATASET_VALID\n",
        "ANNOT_REPR = ANNOT_VALID\n",
        "\n",
        "QUANTIZED_MODEL_NAME = 'nanodet-quant-ppe.keras'\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "N_ITER = 20  # 1 for testing, otherwise 20\n",
        "\n",
        "assert Path(CUSTOM_WEIGHTS_FILE).exists()\n",
        "assert Path(DATASET_REPR).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_PWFm6RX8C5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model(weights=CUSTOM_WEIGHTS_FILE, num_classes=NUM_CLASSES):\n",
        "    INPUT_RESOLUTION = 416\n",
        "    INPUT_SHAPE = (INPUT_RESOLUTION, INPUT_RESOLUTION, 3)\n",
        "    SCALE_FACTOR = 1.5\n",
        "    BOTTLENECK_RATIO = 0.5\n",
        "    FEATURE_CHANNELS = 128\n",
        "\n",
        "    pretrained_weights = torch.load(weights, map_location=torch.device('cpu'))['state_dict']\n",
        "    # Generate Nanodet base model\n",
        "    model = nanodet_plus_m(INPUT_SHAPE, SCALE_FACTOR, BOTTLENECK_RATIO, FEATURE_CHANNELS, num_classes)\n",
        "\n",
        "    # Set the pre-trained weights\n",
        "    load_state_dict(model, state_dict_torch=pretrained_weights)\n",
        "\n",
        "    # Add Nanodet Box decoding layer (decode the model outputs to bounding box coordinates)\n",
        "    scores, boxes = nanodet_box_decoding(model.output, res=INPUT_RESOLUTION, num_classes=num_classes)\n",
        "\n",
        "    # Add TensorFlow NMS layer\n",
        "    outputs = tf.image.combined_non_max_suppression(\n",
        "        boxes,\n",
        "        scores,\n",
        "        max_output_size_per_class=300,\n",
        "        max_total_size=300,\n",
        "        iou_threshold=0.65,\n",
        "        score_threshold=0.001,\n",
        "        pad_per_class=False,\n",
        "        clip_boxes=False\n",
        "        )\n",
        "\n",
        "    model = Model(model.input, outputs, name='Nanodet_plus_m_1.5x_416')\n",
        "\n",
        "    print('Model is ready for evaluation')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H296ybDX8C5"
      },
      "outputs": [],
      "source": [
        "# known warning:  WARNING: head.distribution_project.project not assigned to keras model !!!\n",
        "float_model = get_model(CUSTOM_WEIGHTS_FILE, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yptDvNYNX8C6"
      },
      "source": [
        "# PTQ quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpIdQiGWX8C6"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterator, Tuple, List\n",
        "\n",
        "import cv2\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import coco_dataset_generator, CocoEval\n",
        "\n",
        "def nanodet_preprocess(x):\n",
        "    img_mean = [103.53, 116.28, 123.675]\n",
        "    img_std = [57.375, 57.12, 58.395]\n",
        "    x = cv2.resize(x, (416, 416))\n",
        "    x = (x - img_mean) / img_std\n",
        "    return x\n",
        "\n",
        "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[Tuple]):\n",
        "    def representative_dataset() -> Iterator[List]:\n",
        "        ds_iter = iter(dataset_loader)\n",
        "        for _ in range(n_iter):\n",
        "            yield [next(ds_iter)[0]]\n",
        "\n",
        "    return representative_dataset\n",
        "\n",
        "def quantization(float_model, dataset, annot, n_iter=N_ITER):\n",
        "    # Load representative dataset\n",
        "    representative_dataset = coco_dataset_generator(dataset_folder=dataset,\n",
        "                                                    annotation_file=annot,\n",
        "                                                    preprocess=nanodet_preprocess,\n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "    tpc = mct.get_target_platform_capabilities('tensorflow', 'imx500')\n",
        "\n",
        "    # Preform post training quantization\n",
        "    quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
        "        float_model,\n",
        "        representative_data_gen=get_representative_dataset(n_iter, representative_dataset),\n",
        "        target_platform_capabilities=tpc)\n",
        "\n",
        "    print('Quantized model is ready')\n",
        "    return quant_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJbpVwH3X8C6"
      },
      "outputs": [],
      "source": [
        "quant_model = quantization(float_model, DATASET_REPR, ANNOT_REPR)\n",
        "print(f'Representative dataset: {DATASET_REPR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPpLOtOQX8C6"
      },
      "outputs": [],
      "source": [
        "# Observe that loading quantized model might require specification of custom layers,\n",
        "# see https://github.com/sony/model_optimization/issues/1104\n",
        "quant_model.save(QUANTIZED_MODEL_NAME)\n",
        "print(f'Quantized model saved: {QUANTIZED_MODEL_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTKDafdeX8C6"
      },
      "source": [
        "_todo_: coco evaluation of the custom quantized NanoDet model requires some update to the mct repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqGZPwzX8C6"
      },
      "source": [
        "# Visualize detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZt15gXLX8C6"
      },
      "outputs": [],
      "source": [
        "# Helper functions for visualization\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_image(image_path: str, preprocess: Callable) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load and preprocess an image from a given file path.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        preprocess (function): Preprocessing function to apply to the loaded image.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = preprocess(image)\n",
        "    image = np.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "# draw a single bounding box onto a numpy array image\n",
        "def draw_bounding_box(img, annotation, scale, class_id, score):\n",
        "    row = scale[0]\n",
        "    col = scale[1]\n",
        "    x_min, y_min = int(annotation[1]*col), int(annotation[0]*row)\n",
        "    x_max, y_max = int(annotation[3]*col), int(annotation[2]*row)\n",
        "\n",
        "    color = (0,255,0)\n",
        "\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "    text = f'{int(class_id)}: {score:.2f}'\n",
        "    cv2.putText(img, text, (x_min + 10, y_min + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "# draw all annotation bounding boxes on an image\n",
        "def annotate_image(img, output, scale, quantized_model=False, threshold=0.55):\n",
        "    if quantized_model:\n",
        "        b = output[0].numpy()[0]\n",
        "        s = output[1].numpy()[0]\n",
        "        c = output[2].numpy()[0]\n",
        "    else:\n",
        "        print('Assuming float model')\n",
        "        b = output.nmsed_boxes.numpy()[0]\n",
        "        s = output.nmsed_scores.numpy()[0]\n",
        "        c = output.nmsed_classes.numpy()[0]\n",
        "    for index, row in enumerate(b):\n",
        "        if s[index] > threshold:\n",
        "            #print(f'row: {row}')\n",
        "            id = int(c[index])\n",
        "            draw_bounding_box(img, row, scale, id, s[index])\n",
        "            print(f'class: {CLASS_NAMES[id]} ({id}), score: {s[index]:.2f}')\n",
        "    return {'bbox':b, 'score':s, 'classes':c}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHn2iHp3X8C7"
      },
      "outputs": [],
      "source": [
        "# See appendix for results. For 2 epochs, the bounding boxes are not perfect...\n",
        "# But improves considerably for 20 epochs.\n",
        "test_img = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/image_257_jpg.rf.1a3a6eb456134cce302712c109645c26.jpg'\n",
        "img = load_and_preprocess_image(f'{test_img}', nanodet_preprocess)\n",
        "output = quant_model(img)\n",
        "image = cv2.imread(f'{test_img}')\n",
        "print(f'image shape: {image.shape}')\n",
        "r = annotate_image(image, output, scale=image.shape, quantized_model=True)\n",
        "assert r['score'][0] > 0.5, print(f\"r['score'][0] > 0.5 failed: {r['score'][0]}\")\n",
        "dst = f'annotated.jpg'\n",
        "if cv2.imwrite(dst, image):\n",
        "    print(f'Annotated image saved to: {dst}')\n",
        "else:\n",
        "    print(f'Failed saving annotated image')\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWRq6SEX8C7"
      },
      "source": [
        "# Next step\n",
        "__OBSERVE__: First, save the quantized model to your local machine. You will need it for the conversion and packaging steps.\n",
        "\n",
        "Next step is to convert and package the model for IMX500. _todo: link to further instructions. This model requires bgr settings in the post-converter._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02zmjKnnX8C7"
      },
      "source": [
        "# Appendix\n",
        "## Results total_epochs=2\n",
        "```\n",
        "[NanoDet][07-12 10:47:40]INFO:\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.191\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.026\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.045\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.076\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.100\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.207\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.271\n",
        "\n",
        "[NanoDet][07-12 10:47:40]INFO:\n",
        "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
        "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
        "| Safety-Equipment | nan    | nan   | Person    | 54.1   | 20.1  |\n",
        "| goggles          | 1.6    | 0.4   | helmet    | 40.3   | 13.4  |\n",
        "| no-goggles       | 6.9    | 2.3   | no-helmet | 0.0    | 0.0   |\n",
        "| no-vest          | 3.9    | 1.0   | vest      | 27.1   | 7.9   |\n",
        "[NanoDet][07-12 10:47:40]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
        "[NanoDet][07-12 10:47:40]INFO:Val_metrics: {'mAP': 0.06442128900684024, 'AP_50': 0.1912998265318579, 'AP_75': 0.026441697602184976, 'AP_small': 0.010583883892274994, 'AP_m': 0.044976540581496194, 'AP_l': 0.0756733533889817}\n",
        "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
        "```\n",
        "## Results total_epochs=20\n",
        "```\n",
        "Comments:\n",
        "-  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "-  \"device\": settings for colab T4 GPU\n",
        "-  \"total_epochs\": set to 20 during testing, default 300\n",
        "...\n",
        "[NanoDet][05-16 09:25:43]INFO:\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.610\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.362\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.281\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.479\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n",
        "\n",
        "[NanoDet][05-16 09:25:43]INFO:\n",
        "| class            | AP50   | mAP   | class     | AP50   | mAP   |\n",
        "|:-----------------|:-------|:------|:----------|:-------|:------|\n",
        "| Safety-Equipment | nan    | nan   | Person    | 86.4   | 49.3  |\n",
        "| goggles          | 35.7   | 15.4  | helmet    | 86.0   | 46.3  |\n",
        "| no-goggles       | 42.6   | 16.6  | no-helmet | 34.2   | 13.8  |\n",
        "| no-vest          | 59.6   | 26.3  | vest      | 82.5   | 42.8  |\n",
        "[NanoDet][05-16 09:25:44]INFO:Saving model to workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth\n",
        "[NanoDet][05-16 09:25:44]INFO:Val_metrics: {'mAP': 0.3006027561087712, 'AP_50': 0.6099170448933922, 'AP_75': 0.2496291506747232, 'AP_small': 0.07788513248169772, 'AP_m': 0.212229159695, 'AP_l': 0.36198435595574324}\n",
        "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
        "\n",
        "real\t21m35.722s\n",
        "user\t25m48.699s\n",
        "sys\t6m6.849s\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}